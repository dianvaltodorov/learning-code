# Consultation

## T1

Machine Learning - начин по който машините се самоубучават за да вземат разумно
решение.Дава се информация, може да се даде насока, на база която на
знание вземат решение как да реагират.

## T2

- supervised - някой ни казва как да реагираме в конкретна ситуация
- unsupervised  аналазираме средатат и проблмеа и на база тези
  характеристика без някой отвънка да ни е дал допълнителна информация,
  на база този анализ вземат решение

Пример:

- учене с учител, показвата ни задачи, квадратно уравние, по висок степен, дават 
  ни нова задача, която пак е квадратно ,знанието как се решава се получава от учител
- влизаме в стая с кашон с книги на различни езици. Можем да видим че символите и 
  текста са различни, някои си приличат, едни латиница, други кирилица, виждаме прилики,
  без да са ни дали информация можем да разделим в групи сами да стигнем до извод

Тези проблеми се решават с dataset

има множество инстанциии- задачи по математика, книга, инстаницията се определя с характеристики, атрибути.

a1 a2 a3 a4 ... an | vlass

колко неизвестни има уравнението и някой е минал и е казал имате ли тая задача използвате този подход, 
инстанциите биват причисление към някакъв клас квадратно уравнение

Ученето с учител - класификация(номинална стойност) и регресия(числова стойност)
Имаме учител, който е сетнал

Ученето без учител - клъстеризация, групиране по сходни характеристики
При книгите групираме по език. Това спада към една група това към друга.

## T3

Предварителна обработка на данните - а

- ако на един алгоритъм му се даде некоректна информация, той ще върне некоректен резултат
  да дадем данните в подходящ вид. Ако има липсващи данни да се справим с това,
  Имаме десет републиканции и десет демоктрати и се оказва че има празна стойнстост.
- Игнорираме да намерим средното или най место срещаното и да я зададем под подразбиране
- Липсващи стойност - Махаме въобще тези инстанции ако има такива данни. Обаче
  ако имаме малък dataset не е разумно.
- Справяне с консистентни данни. Понякога данните не могат да вървят едно с друго, 
  премахване на шумове
- Трансформация на данните, вкарваме ги в рамки от 0 до 1.
- Разбиване, събиране, месец и година и отдело час, под атрибути да бъде премахната
- Температура и градуси(0 до 25) има нужда да се образуват непрекъснатия случай
  в дискретен.
- Редуциране на данни, брой атрибути, брой инстанции

## T4

класификация

кнн, дървета, наивен бейс, невронни мрежи, линейна регресия

## T5

кнн, дървета, наивен бейс, невронни мрежи, линейна регресия

клъстеризация

k means, k modes, neuron networks, heiarchal, db scan, expectation maximization


## T6

Невронни мрежи. 

## T7

Разликата между глобални и локални подходи про ибучаване на модел

- глобални - вземаме решение върху целия dataset, например naive bayse
- локални - вземаме решение върху част от dataseta-a,  например knn


Може да се направи сравнение между тези и другите 
Bean Search - взема всички възможни ключа примерно 5 побира три елемента сортира изрязване част от пространството


## T8

- Мързеливо учене - обикновено са локални, има бързо обучение, 
    защото напратика нямаме обучение, не учим докато не дойде изпита. Не изграждаме модел.
    Трябва да пазим целия датасет, и времето на заявката е също много защото трябва да се прави
 Например knn

- Нетърпеливо учене - винаги глобални, Предварителна обработка, изгражда се модел на база целия датасет и
 след това се пази само модел в паметта. Времето за изпълнение на заявката е по малко. например бейс

## T9

Сортира, взема първите К, избира преобладаващия клас, ако има два преобладаващи, взема този който има инстаниция супер близка до входяща


## T10

Дървото се състои.
Всички негови нодове, които не са листа са атрибути. Ребрата които излизат от него биват стойности на атрибутите. 
Поради някаква причина избираме, че атрибута ще е алфа. Листата биват класове

Има специлен начин за избиране на атрибут. За да го направим това трябва да изчислим info gain.
Започваме да филтрираме от най значещото. Не ни е нужно да разгледаме всички примери.

Линка от муудл

Какво правим коагото имаме класове с повече от един от най срещани. Гледаме едно ниво по нагоре.
Ако и там са еднакво срещани, още идно ниво нагоре. Това е по добре.

* id3 - iterative decision tree, работи с дървета с номинални стойности
* c45 оптимизация на id3, работи с числови стойнсти. Например имаме температура
  вместо с Горещо, Топло и Студено, тук разделяме на  интервали. В него е
  имплементирано всякакви оптимизация
* c50 - оптимизация

## T11

проблеми

overfitting - прекомерно нагаждане. Докато тренираме и обучаваме модела
генерираме все повече и повече хипотези, които правят алгорътъма по точне за
нашия трейн.
Прекалено добре за нашия случая, но в общия случай когато му се.
Качваме процент успеваемост за трейна, но влошаваме за общия случай.
Справяме с проблме като правим pre pruning, в момент на строене казваме, че
повече няма да строим, което е доста е турдно. Приетия подход е post pruning.
Режем някоя възможна хипотеза и изследваме подобренията и ни трябва validation
set. Нужно е да се валидира върху някакви данни.


Как избираме хипотези

Occam's Razor

междуконкурентни хипотези трябва да се избере тази с най малко хипотези. Строим максимално просто и компактно Дървото

## T12

Ентропия - мярка на несигурността на случайна променлива.
Колкото по малка е ентропията, толкова по сигурна е информацията, която имаме.
Какво става при ентропия 0 и какво става при ентропия 1. При 0 винаги имаме Yes.
Когато имаме две възможности с еднаква вероятност е 1

Гейн - функция на печалба, най малката оставаща eнтропия след теста. Колко
значима е една информация.

р,бва задължително знаем формула на бейс и изчисление на ентропия. И как да ги
прилагаме при даден сет. Много е важно!!!

Е(1, 1) = 1

E(4, 0) = 0

Е(2, 3) = E(3, 2)

## T13

Разписване на доказателството за Бейс.

## T14

Имаме пространство в което изобразяваме инстанциите. Сетваме центроиди на произволен принцип.
Гледаме всеки елемент до кой центроид е най-близък и причисляваме към този клас. Получаваме два клъстера.
След това тези центроди изчезват и почваме сладващата итерация.
Избираме за нови центроиди равноотдалечената точка от всички инстанции за дадения клас.
Повтаряме  докато стигнем в ситуация. Терминираме като спрам да имаме прехвърляне от клъстър в клъстър. 

Този алгорътм е локален. Намира се локален оптимум. А при локално търсещите алогиритми се справяме с 
платата пускаме рандом и избираме най-добрия надяваме се че ще открием. Не е задължително да имаме най-добро решение

Алгоритъмът не е йерархичен.

Дадени са уравнения, дали едно преобразуване е коректно.

## T15

Йерархичните алгоритми изгражда дендограми, йерархия, която прилича на дърво.
Два вида top down bottom up.

- Devisive approach, top down - всички инстанции са един голям клъстер. Например bisecting k-means. След което казваме разбий този на два подклъстера.
- Агломеративен approach, bottom up - пространство от точки. Всеки един елемент е клъстер. и почваме да обединяваме и групираме.
  Винаги търсим близост. Въпроса е как оценяваме близост. Начин
    - single link - гледаме кое разстояние е най близо.
    - complete link  - гледаме най далечната
    - average link - гледаме средно аритметичното.


## T16 and T17

Невронните мрежи

- еднослойни - имат вход и изход, решават линейно разделими проблеми,
  ако имаме преплитане и имаме нужда от крива, няма да стане,
  затова ползваме многослойни.
- многослойни - изпъкнати повърхности, and or xor графично булевите функции примат две числа 0 и 1
  xor е линейно неразделим е еднослойно


персетрони са еднослойни невронни мрежи имат стъпкова функция(недиференцируема)

    правови
    безправови
    линейнени възли
    сигнуиди

    идеята за епохи.
Цялостната идея, че един неврон има стойност.
Всеки един неврон има връзка с някой от предходния слой.

    рекуренти
    forward-checking

## T18

Backpropagation не е невронна мрежа, а е начин за обучение на невронни мрежи.
След като се изчисли грешката, която имаме и връщаме лейър по лейър нагоре
Използваме получената грешката за да се учим от нея и update-ваме

## T19

Асоциативни правила. Четеш по десет пъти!!!!

Дефиниция:

Учене обосновано на асоциативни правила е метод за откриване
на интересни връзки между променливи в големи бази данни.
Предназначено е за откриването и идентифицирането на строги правила,
открити в базата данни използвайки методи за оценяване на интереси.
За да изберете интересни правила от множеството от всички правила се използват ограничения върху различните мерки за оценяване на значението и интереса. 
Най добре познати ограничения са минималния праг на поддръжка (означение колко често дадена група или itemset присъства в базата данни)
и минимален праг на доверие - означение колко често дадено правило е установено като истина.


Едно нещо следва от нещо друго.

## T20

Apriori

## T21

Типове разделения
    train set - обучаваме алгоритъма
    validation set - настройваме нашия алгоритъм. пускаме го да видим как се представя.
                    Пример овърфитинг, процента успеваемост е много лош, дърветата на решения обрязваме, кнн сменяме к, настройваме алгоритъма, 
                    тунинговаме го да работи в общия случай. И се надяваме каквото и да дойде в общия случай ще върне.
                    Пример за валидейшън сет е крос валидейшън. 
    test set - пускаме в реална среда.

## T22

Оценка на алгоритъм

Търсене в гугл. Има 7 правилни. Връща 3 срещи това да връща 10.

Пълен recall, пълен Precision.
F measure.


- Точност
- метрики с булево
- Precision
- Maximal
- Сравнение на алгоритми

## T23

K fold corss validation-

Разбиваме нашето множество на части. За всяко едно нещо изкарваме процент успеваемост и накрая усредняваме
Крос валидация се използва когато искаме да направим валидация и също така когато нашия сет е малък.

## T24

## Other:

Ансамбално обучение

Разбиваме трейн на няколко подчасти. Този алгоритъм ще върне различен резултат. 
Гласуване. Един алгоритъм казва едно, други казва друго.
NLP



